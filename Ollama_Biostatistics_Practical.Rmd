---
title: "Local LLMs for Biostatistics: Practical Guide"
subtitle: "Using Ollama + R for Statistical Analysis"
author: "Amir Javid"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
    theme: flatly
    code_folding: show
    highlight: tango
    df_print: paged
    fig_caption: true
    number_sections: true
---

```{css custom-styles, echo=FALSE}
/* Custom Styling */
.main-container { max-width: 1100px; }

/* Hero Section */
.hero-box {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  padding: 30px;
  border-radius: 10px;
  margin-bottom: 30px;
}
.hero-box h2 { color: white; margin-top: 0; }

/* Info Boxes */
.info-box {
  background-color: #e7f3ff;
  border-left: 5px solid #2196F3;
  padding: 15px 20px;
  margin: 20px 0;
  border-radius: 0 5px 5px 0;
}
.tip-box {
  background-color: #e8f5e9;
  border-left: 5px solid #4CAF50;
  padding: 15px 20px;
  margin: 20px 0;
  border-radius: 0 5px 5px 0;
}
.warning-box {
  background-color: #fff3e0;
  border-left: 5px solid #FF9800;
  padding: 15px 20px;
  margin: 20px 0;
  border-radius: 0 5px 5px 0;
}

/* Better Code Output */
pre code {
  font-size: 13px;
  line-height: 1.5;
}

/* Model Table Styling */
.model-table {
  width: 100%;
  border-collapse: collapse;
}
.model-table th {
  background-color: #667eea;
  color: white;
  padding: 12px;
}
.model-table td {
  padding: 10px;
  border-bottom: 1px solid #ddd;
}
.model-table tr:hover { background-color: #f5f5f5; }

/* Section Headers */
h1 { border-bottom: 3px solid #667eea; padding-bottom: 10px; }
h2 { color: #667eea; }

/* Output Styling */
.llm-output {
  background: linear-gradient(135deg, #f8f9fa 0%, #fff 100%);
  border: 1px solid #e2e8f0;
  border-left: 4px solid #667eea;
  border-radius: 0 12px 12px 0;
  padding: 20px 25px;
  margin: 20px 0;
  font-size: 15px;
  line-height: 1.8;
  color: #2d3748;
  box-shadow: 0 2px 8px rgba(0,0,0,0.05);
}
.llm-output h1, .llm-output h2, .llm-output h3, 
.llm-output h4, .llm-output h5, .llm-output h6 {
  color: #4a5568;
  margin-top: 1em;
  margin-bottom: 0.5em;
}
.llm-output ul, .llm-output ol {
  padding-left: 1.5em;
  margin: 0.5em 0;
}
.llm-output li {
  margin: 0.3em 0;
}
.llm-output code {
  background-color: #edf2f7;
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 0.9em;
}
.llm-output strong {
  color: #2b6cb0;
}
.llm-output blockquote {
  border-left: 3px solid #cbd5e0;
  padding-left: 1em;
  margin: 1em 0;
  color: #718096;
}

/* Clean Output Blocks */
pre:not(.sourceCode) {
  background-color: #f8f9fa;
  border-left: 4px solid #667eea;
  padding: 15px 20px;
  margin: 15px 0;
  border-radius: 0 8px 8px 0;
  white-space: pre-wrap;
  word-wrap: break-word;
  font-family: 'Segoe UI', system-ui, sans-serif;
  font-size: 14px;
  line-height: 1.7;
  color: #333;
}

/* Markdown-style formatting in outputs */
pre:not(.sourceCode) strong,
pre:not(.sourceCode) b {
  font-weight: 600;
}

/* Better list rendering in text output */
pre:not(.sourceCode) {
  overflow-x: auto;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = FALSE,
  comment = ""
)

# Set eval to TRUE for chunks you want to run
run_llm <- TRUE  # Change to TRUE to execute LLM calls
```

<div class="hero-box">
<h2>üß¨ Bring AI to Your Biostatistics Workflow</h2>
<p>This guide demonstrates how to use <strong>local Large Language Models</strong> (via Ollama) for statistical consulting, code generation, and analysis design‚Äîall running privately on your machine.</p>

**What you'll learn:**

- ‚úÖ Statistical consultation with AI experts
- ‚úÖ Automated R code generation for complex analyses
- ‚úÖ Iterative analysis design workflows
- ‚úÖ Semantic literature search with embeddings
- ‚úÖ Structured outputs for reproducible reports
</div>

<div class="info-box">
**üìã Quick Reference:** Jump to any section using the floating table of contents on the left.
</div>

# Quick Start

Install Ollama from [ollama.com](https://ollama.com), launch the app, then:

```{r install, eval=FALSE}
install.packages("ollamar")
library(ollamar)
test_connection()  # Should show "Ollama server is running"

# Pull your models (one-time setup)
pull("qwen3:14b")          # Best for R code generation
pull("gemma3:12b")         # Best for complex reasoning  
pull("gemma3:4b")          # Fast general-purpose
pull("llama3.2:3b")        # Quick explanations
```

**Available Models:**

| Model | Size | Best Use | Speed |
|-------|------|----------|-------|
| `qwen3:14b` | 9.3GB | R code generation, complex analysis | ‚ö°‚ö° |
| `gemma3:12b` | 8.1GB | Complex reasoning, study design | ‚ö°‚ö° |
| `qwen3:4b` | 2.5GB | Quick code questions | ‚ö°‚ö°‚ö° |
| `gemma3:4b` | 3.3GB | Fast reasoning, general tasks | ‚ö°‚ö°‚ö° |
| `phi4-mini:3.8b` | 2.5GB | Efficient general-purpose | ‚ö°‚ö°‚ö° |
| `granite4:3b` | 2.1GB | IBM's enterprise model | ‚ö°‚ö°‚ö° |
| `llama3.2:3b` | 2.0GB | Fast explanations, summaries | ‚ö°‚ö°‚ö° |
| `nomic-embed-text-v2-moe` | 957MB | Embeddings, semantic search | ‚ö°‚ö°‚ö° |
| `mxbai-embed-large` | 669MB | Embeddings, literature search | ‚ö°‚ö°‚ö° |

---

# Core Workflow Patterns

<div class="tip-box">
**üí° Pro Tip:** Each pattern below can be combined. Start with consultation (Pattern 1), then generate code (Pattern 2), and iterate to refine (Pattern 3).
</div>

## Pattern 1: Statistical Consultation

Get expert advice on methods and tests:

```{r consultation, eval=run_llm, results='asis'}
library(ollamar)

# Setup: System prompt for biostatistics expert
expert_prompt <- "You are a senior biostatistician. Give concise, evidence-based recommendations with key assumptions. Use markdown formatting."

# Query function that formats output for R Markdown
ask_expert <- function(question, model = "gemma3:12b", show = TRUE) {
  msgs <- create_messages(
    create_message(expert_prompt, role = "system"),
    create_message(question)
  )
  response <- chat(model, msgs, 
       temperature = 0.3, num_predict = 800,
       output = "text")
  # Wrap in a styled div for better presentation
  if (show) {
    cat("\n<div class='llm-output'>\n\n")
    cat(response)
    cat("\n\n</div>\n")
  }
  invisible(response)
}

# Example: Which test?
ask_expert("
Study: RCT, 3 treatment arms (n=40 each)
Outcome: Pain reduction (0-10 scale, skewed right)
Question: Best statistical test and why?
")
```

## Pattern 2: R Code Generation

Generate analysis code with proper parameters:

```{r codegen, eval=run_llm, results='asis'}
# Code generation function with formatted output
generate_code <- function(prompt, model = "qwen3:14b", num_predict = 2000, show = TRUE) {
  response <- generate(
    model,
    prompt,
    temperature = 0.2,      # Low for deterministic code
    top_p = 0.3,
    num_predict = num_predict,
    output = "text"
  )
  if (show) {
    cat("\n<div class='llm-output'>\n\n")
    cat(response)
    cat("\n\n</div>\n")
  }
  invisible(response)
}

# Example: Mixed model code
generate_code("
Write R code for mixed-effects model:
- Data: longitudinal BP measurements (n=200 patients, 4 timepoints)
- Fixed: treatment (2 levels), time, treatment*time
- Random: patient intercept
- Use lme4, include model diagnostics
- Keep code under 50 lines
")
```

## Pattern 3: Iterative Analysis Design

Build complex analyses step-by-step:

```{r iterative, eval=run_llm, results='asis'}
# Start conversation
conv <- create_messages(
  create_message("You are a biostatistics consultant guiding analysis design. Use markdown formatting.", role = "system"),
  create_message("Design primary analysis for Phase 3 hypertension trial comparing Drug A vs Placebo")
)

# Step 1: Get approach
step1 <- chat("gemma3:12b", conv, output = "text")
cat("\n### üìã Analysis Approach\n\n<div class='llm-output'>\n\n")
cat(step1)
cat("\n\n</div>\n")

# Step 2: Add details, continue conversation
conv <- append_message(step1, role = "assistant", x = conv)
conv <- append_message("Primary endpoint: Change in SBP at 12 weeks. Baseline SBP ~145mmHg (SD=15). n=300 total.", 
                       role = "user", x = conv)

step2 <- chat("gemma3:12b", conv, output = "text")
cat("\n### üìä Detailed Plan\n\n<div class='llm-output'>\n\n")
cat(step2)
cat("\n\n</div>\n")

# Step 3: Get code (switch to code model)
conv <- append_message(step2, role = "assistant", x = conv)
conv <- append_message("Provide complete R code for this analysis", role = "user", x = conv)

code <- chat("qwen3:14b", conv, output = "text")
cat("\n### üíª Generated Code\n\n<div class='llm-output'>\n\n")
cat(code)
cat("\n\n</div>\n")
```

---

# Practical Biostatistics Examples

<div class="info-box">
**üéØ Real-World Scenarios:** These examples mirror common biostatistics tasks in clinical trials and observational studies. Each generates complete, runnable R code.
</div>

## Sample Size & Power

```{r power, eval=run_llm, results='asis'}
power_query <- "
Trial design:
- Outcome: HbA1c reduction (continuous, SD=1.2%)
- MCID: 0.5% difference between groups
- Power: 90%, Œ±=0.05 (two-sided)
- Expected dropout: 15%

Calculate sample size per group. Provide R code using pwr package.
"

generate_code(power_query)
```

## Survival Analysis Pipeline

```{r survival, eval=run_llm, results='asis'}
generate_code("
Create survival analysis for cancer trial (n=250):
1. Generate realistic data: time (months), status (0/1), treatment (A/B), age, stage
2. Kaplan-Meier curves by treatment
3. Log-rank test
4. Cox model with treatment + age + stage
5. Check PH assumption
6. Forest plot of HRs
Make it publication-ready. Use survival and survminer packages.
")
```

## Missing Data Strategy

```{r missing, eval=run_llm, results='asis'}
ask_expert("
Longitudinal trial, 4 visits, ~20% dropout by visit 4 (likely MNAR).
Primary: Change from baseline at visit 4.
Options: MMRM, MI, LOCF, complete case?
Recommend primary + sensitivity analyses.
")
```

## Propensity Score Analysis

```{r psm, eval=run_llm, results='asis'}
generate_code("
Observational study comparing treatment effect:
- Outcome: 30-day mortality (binary)
- Treatment: Surgery vs Medical (n=800 each)
- Confounders: age, comorbidities, severity score

Complete PS analysis:
1. Estimate propensity scores (logistic)
2. Check balance before/after matching
3. 1:1 matching with caliper
4. Estimate treatment effect
5. Sensitivity to hidden confounding
Use MatchIt and sensemakr packages. Commented code.
")
```

---

# Parallel Processing for Efficiency

<div class="warning-box">
**‚ö†Ô∏è Performance Note:** Parallel requests can speed up batch queries significantly, but monitor your system resources. Ollama loads models into GPU/RAM, so running too many concurrent requests may cause memory issues.
</div>

Process multiple queries simultaneously:

```{r parallel, eval=run_llm, results='asis'}
library(httr2)

# Multiple statistical questions
questions <- c(
  "When to use Wilcoxon vs t-test?",
  "Interpret odds ratio of 2.5 in logistic regression",
  "Difference between ITT and PP analysis",
  "When to use Bonferroni vs FDR correction"
)

# Create parallel requests (fast model)
reqs <- lapply(questions, function(q) {
  generate("gemma3:4b", q, 
           temperature = 0.3, num_predict = 300,
           output = "req")
})

# Execute in parallel
resps <- req_perform_parallel(reqs, on_error = "continue")

# Process results
answers <- lapply(resps, resp_process, "text")

# Display with styled boxes
for(i in seq_along(questions)) {
  cat(sprintf("\n**Q%d: %s**\n\n<div class='llm-output'>\n\n%s\n\n</div>\n", 
              i, questions[i], answers[[i]]))
}
```

---

# Literature Search with Embeddings

Semantic search through your literature database:

```{r embeddings, eval=run_llm}
# Your abstracts database
abstracts <- c(
  "Cox proportional hazards analysis of survival in stage III colon cancer patients treated with adjuvant chemotherapy.",
  "Propensity score matching to evaluate effectiveness of drug-eluting stents vs bare-metal stents in real-world setting.",
  "Mixed-effects model for repeated measures (MMRM) in longitudinal clinical trials with missing data.",
  "Bayesian adaptive design for phase II oncology trials with continuous monitoring of efficacy and toxicity.",
  "Meta-analysis of randomized trials comparing intensive vs standard glycemic control in type 2 diabetes."
)

# Generate embeddings
embs <- lapply(abstracts, function(x) embed("mxbai-embed-large", x))

# Search function
search_literature <- function(query, abstracts, embeddings, top_n = 3) {
  query_emb <- embed("mxbai-embed-large", query)
  
  # Cosine similarity
  sims <- sapply(embeddings, function(e) sum(e * query_emb))
  
  # Return top results
  idx <- order(sims, decreasing = TRUE)[1:top_n]
  data.frame(
    rank = 1:top_n,
    similarity = round(sims[idx], 3),
    abstract = abstracts[idx]
  )
}

# Example search
results <- search_literature("survival analysis Cox regression", abstracts, embs)
print(results)
```

---

# Structured Output for Reports

Get responses in structured format:

```{r structured, eval=run_llm, results='asis'}
# Define schema (JSON schema as R list)
analysis_schema <- list(
  type = "object",
  properties = list(
    test_name = list(type = "string"),
    assumptions = list(type = "array", items = list(type = "string")),
    r_function = list(type = "string"),
    interpretation = list(type = "string")
  ),
  required = list("test_name", "assumptions", "r_function")
)

# Get structured response (use output = "structured")
result <- chat(
  "qwen3:14b",
  create_message("Recommend a statistical test for comparing 3 independent groups with an ordinal outcome (5-point Likert scale). Return the test name, key assumptions, R function to use, and interpretation guidance."),
  format = analysis_schema,
  output = "structured"
)

# Display structured result nicely
cat("\n<div class='llm-output'>\n\n")
cat("**Test:**", result$test_name, "\n\n")
cat("**R Function:**", paste0("`", result$r_function, "`"), "\n\n")
cat("**Assumptions:**\n\n")
for(a in result$assumptions) cat("- ", a, "\n")
cat("\n**Interpretation:**\n\n", result$interpretation, "\n\n")
cat("</div>\n")
```

---

# Real Data Example: Complete Analysis

<div class="tip-box">
**üî¨ Hands-On Demo:** This section uses R's built-in `mtcars` dataset to demonstrate a complete LLM-assisted analysis workflow from exploratory analysis to interpretation.
</div>

```{r real-data-example, eval=run_llm, results='asis'}
# Load and summarize real data
data(mtcars)

# Create analysis context for LLM
data_summary <- sprintf("
Dataset: mtcars (Motor Trend Car Road Tests)
Observations: %d cars
Variables: mpg (fuel efficiency), cyl (cylinders: 4,6,8), hp (horsepower), wt (weight), am (transmission: 0=auto, 1=manual)

Summary statistics:
- MPG: mean=%.1f, sd=%.1f, range=[%.1f, %.1f]
- Cylinders: %d four-cyl, %d six-cyl, %d eight-cyl
- Transmission: %d automatic, %d manual

Research question: Does transmission type (automatic vs manual) affect fuel efficiency (mpg), controlling for weight and horsepower?
", nrow(mtcars), mean(mtcars$mpg), sd(mtcars$mpg), min(mtcars$mpg), max(mtcars$mpg),
sum(mtcars$cyl==4), sum(mtcars$cyl==6), sum(mtcars$cyl==8),
sum(mtcars$am==0), sum(mtcars$am==1))

# Ask LLM for analysis recommendation
cat("\n### üìä Analysis Plan\n")
ask_expert(paste0(data_summary, "
Recommend the appropriate statistical analysis approach. Consider:
1. Which test/model to use and why
2. Key assumptions to check
3. Potential confounders
4. How to interpret results
"))

# Generate analysis code
cat("\n### üíª Generated R Code\n")
generate_code(paste0(data_summary, "
Write complete R code to:
1. Visualize the relationship (boxplot of mpg by transmission)
2. Fit appropriate model (multiple regression: mpg ~ am + wt + hp)
3. Check model assumptions (residual plots)
4. Report results with confidence intervals
Use base R and keep it concise.
"))
```

---

# Helper Functions

Reusable utilities for your workflow:

```{r helpers}
# Quick consultation
quick_ask <- function(question, model = "llama3.2:3b") {
  generate(model, question, 
           temperature = 0.3, num_predict = 500,
           output = "text")
}

# Safe code generation with retry
safe_codegen <- function(prompt, model = "qwen3:14b", max_retry = 3) {
  for(i in 1:max_retry) {
    result <- tryCatch(
      generate_code(prompt, model),
      error = function(e) {
        message(sprintf("Attempt %d failed: %s", i, e$message))
        if(i < max_retry) Sys.sleep(2)
        NULL
      }
    )
    if(!is.null(result)) return(result)
  }
  stop("Code generation failed after retries")
}

# Multi-model consensus
get_consensus <- function(question, models = c("gemma3:12b", "qwen3:14b")) {
  results <- lapply(models, function(m) {
    cat(sprintf("\n=== %s ===\n", m))
    resp <- quick_ask(question, m)
    cat(resp, "\n")
    resp
  })
  invisible(results)
}
```

**Usage examples:**

```{r helper-usage, eval=FALSE}
# Quick question
quick_ask("What's the formula for Cohen's d?")

# Safe code generation with auto-retry
code <- safe_codegen("Create function to calculate 95% CI for proportion")

# Get multiple model opinions
get_consensus("Should I adjust for baseline in ANCOVA for RCT?")
```

---

# Model Selection Guide

**Choose based on task complexity and speed needs:**

```{r model-selector}
select_model <- function(task, need_speed = FALSE) {
  switch(task,
    "code" = if(need_speed) "gemma3:4b" else "qwen3:14b",
    "reasoning" = "gemma3:12b",
    "quick" = "llama3.2:3b",
    "vision" = "qwen3-vl:8b",
    "embed" = "mxbai-embed-large",
    "llama3.2:3b"  # default
  )
}

# Example
model <- select_model("code", need_speed = FALSE)
cat("Selected model:", model)
```

---

# Best Practices

## 1. Verify Everything

```{r verify, eval=FALSE}
# Always test generated code
code <- generate_code("Create t-test function")
# Review before running!
# eval(parse(text = code))

# Cross-check statistical advice
advice1 <- ask_expert("...", "gemma3:12b")
advice2 <- ask_expert("...", "qwen3:14b")
# Compare responses
```

## 2. Provide Rich Context

```{r context-example, eval=FALSE}
# Bad: Vague question
"What test should I use?"

# Good: Specific context
"
Study: 2-arm RCT (n=100 per arm)
Outcome: QoL score 0-100 (continuous, normally distributed)
Baseline: Measured
Analysis: ANCOVA adjusting for baseline
Question: Should I transform outcome? How to report?
"
```

## 3. Optimize Parameters

```{r params}
# Task-specific parameters (pass directly to functions)
# Code generation: low temp, high tokens for complete output
# generate(..., temperature = 0.2, top_p = 0.3, num_predict = 2000-3000)

# Reasoning: moderate temperature
# generate(..., temperature = 0.4, top_p = 0.5, num_predict = 1500)

# Quick answers: lower tokens
# generate(..., temperature = 0.3, num_predict = 500-1000)

# Complex tasks (full analysis): use higher num_predict to avoid truncation
# generate(..., temperature = 0.2, num_predict = 3000)

# Example usage
generate("qwen3:14b", "...", temperature = 0.2, num_predict = 2000, output = "text")
```

---

# Troubleshooting

```{r troubleshoot, eval=FALSE}
# Connection issues
test_connection()
# If failed: restart Ollama app

# Model not found
list_models()  # Check what you have
pull("qwen3:14b")  # Pull if missing

# Out of memory
# Use smaller model:
generate("gemma3:4b", "...", output = "text")

# Slow performance
# Reduce output length:
generate("llama3.2:3b", "...", 
         num_predict = 500, output = "text")
```

---

# Real Example: Complete Analysis Workflow

From design to code:

```{r complete-example, eval=run_llm, results='asis'}
# Step 1: Design consultation
cat("\n### üìã Step 1: Design Consultation\n")
design <- ask_expert("
Trial: compare 3 doses of new antihypertensive
Primary: SBP change at week 12
Secondary: DBP change, response rate (SBP<140)
Covariates: baseline SBP, age, BMI
Sample: n=75 per group
Recommend primary analysis approach
", show = FALSE)  # Capture without showing

cat("\n<div class='llm-output'>\n\n", design, "\n\n</div>\n")

# Step 2: Generate analysis code (use higher token limit for complex code)
cat("\n### üíª Step 2: Analysis Code\n")
generate_code(sprintf("
Based on this design:
%s

Create R code for:
1. Data simulation (realistic)
2. Primary analysis (ANCOVA)
3. Secondary analyses
4. Table 1
5. Publication-quality plots
Use tidyverse + emmeans. Well-commented.
", design), num_predict = 3000)

# Step 3: Generate reporting template (also needs more tokens)
cat("\n### üìù Step 3: Report Template\n")
generate_code("
Create R Markdown template for reporting the analysis above.
Include: Methods section, Results section with inline R code,
Tables and figures. Follow CONSORT style.
", num_predict = 2500)
```

---

# Resources & References

<div class="info-box">
**üìö Learn More:** Explore these resources to deepen your understanding of local LLMs and biostatistics with R.
</div>

## Official Documentation

| Resource | Description |
|----------|-------------|
| [ollamar Package](https://hauselin.github.io/ollama-r/) | Official R package documentation |
| [Ollama Models](https://ollama.com/library) | Browse available models |
| [Ollama Blog](https://ollama.com/blog) | Latest features and tutorials |

## Recommended Reading

- **Structured Outputs**: [ollama.com/blog/structured-outputs](https://ollama.com/blog/structured-outputs)
- **Tool Calling**: [ollama.com/blog/tool-support](https://ollama.com/blog/tool-support)
- **Model Selection Guide**: Choose models based on your task complexity

## Citation

If you use ollamar in your research, please cite:

```bibtex
@article{Lin2025,
  author = {Lin, Hause and Safi, Tawab},
  title = {ollamar: An R package for running large language models},
  journal = {Journal of Open Source Software},
  volume = {10},
  number = {105},
  pages = {7211},
  year = {2025},
  doi = {10.21105/joss.07211}
}
```

---

# Session Information

<details>
<summary>Click to expand session info</summary>

```{r session-info}
sessionInfo()
```

</details>

---

<div style="text-align: center; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; margin-top: 30px;">
<h3 style="color: white; margin: 0;">üéâ Thank You for Exploring!</h3>
<p>Found this useful? Star the repository on GitHub!</p>
<p><a href="https://github.com/ahjavid/R-Ollama-Lab" style="color: #fff; text-decoration: underline;">github.com/ahjavid/R-Ollama-Lab</a></p>
</div>
